{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import regularizers\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>GENDER</th>\n",
       "      <th>DOB</th>\n",
       "      <th>DOD</th>\n",
       "      <th>DOA</th>\n",
       "      <th>ADMIT_AGE</th>\n",
       "      <th>ETHNICITY</th>\n",
       "      <th>MARITAL_STATUS</th>\n",
       "      <th>LANGUAGE</th>\n",
       "      <th>...</th>\n",
       "      <th>VALUENUM-8-7-6-5-4-3-2</th>\n",
       "      <th>ITEMID-3</th>\n",
       "      <th>VALUENUM-8-7-6-5-4-3</th>\n",
       "      <th>ITEMID</th>\n",
       "      <th>VALUENUM-8-7-6-5-4</th>\n",
       "      <th>VALUENUM-8-7-6-5</th>\n",
       "      <th>VALUENUM-8-7-6</th>\n",
       "      <th>VALUENUM-8-7</th>\n",
       "      <th>VALUENUM-8</th>\n",
       "      <th>VALUENUM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>31</td>\n",
       "      <td>M</td>\n",
       "      <td>2036-05-17</td>\n",
       "      <td>2108-08-30 00:00:00</td>\n",
       "      <td>2108-08-22 23:27:00</td>\n",
       "      <td>72.312329</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>MARRIED</td>\n",
       "      <td>UKNOWN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.3</td>\n",
       "      <td>33.9</td>\n",
       "      <td>11.0</td>\n",
       "      <td>26.3</td>\n",
       "      <td>260.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>56</td>\n",
       "      <td>F</td>\n",
       "      <td>1804-01-02</td>\n",
       "      <td>2104-01-08 00:00:00</td>\n",
       "      <td>2104-01-02 02:01:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>UKNOWN</td>\n",
       "      <td>UKNOWN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.2</td>\n",
       "      <td>24.5</td>\n",
       "      <td>7.1</td>\n",
       "      <td>8.2</td>\n",
       "      <td>124.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>207</td>\n",
       "      <td>61</td>\n",
       "      <td>M</td>\n",
       "      <td>2063-10-21</td>\n",
       "      <td>2119-02-03 00:00:00</td>\n",
       "      <td>2119-01-04 18:12:00</td>\n",
       "      <td>55.241096</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>MARRIED</td>\n",
       "      <td>UKNOWN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.2</td>\n",
       "      <td>13.1</td>\n",
       "      <td>4.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>208</td>\n",
       "      <td>67</td>\n",
       "      <td>M</td>\n",
       "      <td>2084-06-05</td>\n",
       "      <td>2157-12-02 00:00:00</td>\n",
       "      <td>2157-12-02 00:45:00</td>\n",
       "      <td>73.539726</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>SINGLE</td>\n",
       "      <td>UKNOWN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.2</td>\n",
       "      <td>54.1</td>\n",
       "      <td>18.4</td>\n",
       "      <td>9.3</td>\n",
       "      <td>183.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>214</td>\n",
       "      <td>84</td>\n",
       "      <td>F</td>\n",
       "      <td>2151-10-21</td>\n",
       "      <td>2196-04-17 00:00:00</td>\n",
       "      <td>2196-04-14 04:02:00</td>\n",
       "      <td>44.512329</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>MARRIED</td>\n",
       "      <td>UKNOWN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.9</td>\n",
       "      <td>36.3</td>\n",
       "      <td>12.6</td>\n",
       "      <td>7.1</td>\n",
       "      <td>262.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  SUBJECT_ID GENDER         DOB                  DOD  \\\n",
       "0          10          31      M  2036-05-17  2108-08-30 00:00:00   \n",
       "1          14          56      F  1804-01-02  2104-01-08 00:00:00   \n",
       "2         207          61      M  2063-10-21  2119-02-03 00:00:00   \n",
       "3         208          67      M  2084-06-05  2157-12-02 00:00:00   \n",
       "4         214          84      F  2151-10-21  2196-04-17 00:00:00   \n",
       "\n",
       "                   DOA  ADMIT_AGE ETHNICITY MARITAL_STATUS LANGUAGE  ...  \\\n",
       "0  2108-08-22 23:27:00  72.312329     WHITE        MARRIED   UKNOWN  ...   \n",
       "1  2104-01-02 02:01:00        NaN     WHITE         UKNOWN   UKNOWN  ...   \n",
       "2  2119-01-04 18:12:00  55.241096     WHITE        MARRIED   UKNOWN  ...   \n",
       "3  2157-12-02 00:45:00  73.539726     WHITE         SINGLE   UKNOWN  ...   \n",
       "4  2196-04-14 04:02:00  44.512329     WHITE        MARRIED   UKNOWN  ...   \n",
       "\n",
       "  VALUENUM-8-7-6-5-4-3-2 ITEMID-3 VALUENUM-8-7-6-5-4-3  ITEMID  \\\n",
       "0                    0.0      0.0                  0.0     0.0   \n",
       "1                    0.0      0.0                  0.0     0.0   \n",
       "2                    0.0      0.0                  0.0     0.0   \n",
       "3                    0.0      0.0                  0.0     0.0   \n",
       "4                    0.0      0.0                  0.0     0.0   \n",
       "\n",
       "   VALUENUM-8-7-6-5-4  VALUENUM-8-7-6-5  VALUENUM-8-7-6  VALUENUM-8-7  \\\n",
       "0                 0.0              13.3            33.9          11.0   \n",
       "1                 0.0              13.2            24.5           7.1   \n",
       "2                 0.0              12.2            13.1           4.2   \n",
       "3                 0.0              13.2            54.1          18.4   \n",
       "4                 0.0              12.9            36.3          12.6   \n",
       "\n",
       "   VALUENUM-8  VALUENUM  \n",
       "0        26.3     260.0  \n",
       "1         8.2     124.0  \n",
       "2         0.4      11.0  \n",
       "3         9.3     183.0  \n",
       "4         7.1     262.0  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographics = pd.read_csv('Demographics-new.csv')\n",
    "demographics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics.fillna(0, inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define AKI diagnoses from non-AKI diagnoses\n",
    "aki_pre = demographics[demographics['OLD_FLAG']==0]\n",
    "aki_pos = aki_pre[aki_pre['AKI_DIAGNOSIS_FLAG']== 1]\n",
    "aki_neg = aki_pre[aki_pre['AKI_DIAGNOSIS_FLAG']==0]\n",
    "\n",
    "# Clean data sets\n",
    "del aki_neg['CAUSE']\n",
    "del aki_pos['CAUSE']\n",
    "\n",
    "del aki_neg['AKI_DIAGNOSIS_FLAG']\n",
    "del aki_pos['AKI_DIAGNOSIS_FLAG']\n",
    "\n",
    "del aki_neg['OLD_FLAG']\n",
    "del aki_pos['OLD_FLAG']\n",
    "\n",
    "del aki_neg['OUTSIDE_DEATH_FLAG']\n",
    "del aki_pos['OUTSIDE_DEATH_FLAG']\n",
    "\n",
    "del aki_neg['SUBJECT_ID']\n",
    "del aki_pos['SUBJECT_ID']\n",
    "\n",
    "del aki_neg['DOB']\n",
    "del aki_pos['DOB']\n",
    "\n",
    "del aki_neg['DOD']\n",
    "del aki_pos['DOD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GENDER</th>\n",
       "      <th>ADMIT_AGE</th>\n",
       "      <th>ETHNICITY</th>\n",
       "      <th>MARITAL_STATUS</th>\n",
       "      <th>LANGUAGE</th>\n",
       "      <th>RELIGION</th>\n",
       "      <th>INSURANCE</th>\n",
       "      <th>ADMISSION_LOCATION</th>\n",
       "      <th>#ADMISSIONS</th>\n",
       "      <th>DEATH_FLAG</th>\n",
       "      <th>...</th>\n",
       "      <th>LIVER_FLAG</th>\n",
       "      <th>VALUENUM-8-7-6-5-4-3-2-1</th>\n",
       "      <th>VALUENUM-8-7-6-5-4-3-2</th>\n",
       "      <th>VALUENUM-8-7-6-5-4-3</th>\n",
       "      <th>VALUENUM-8-7-6-5-4</th>\n",
       "      <th>VALUENUM-8-7-6-5</th>\n",
       "      <th>VALUENUM-8-7-6</th>\n",
       "      <th>VALUENUM-8-7</th>\n",
       "      <th>VALUENUM-8</th>\n",
       "      <th>VALUENUM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>M</td>\n",
       "      <td>80.564384</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>SINGLE</td>\n",
       "      <td>UKNOWN</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>Medicare</td>\n",
       "      <td>EMERGENCY ROOM ADMIT</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>25.5</td>\n",
       "      <td>9.1</td>\n",
       "      <td>3.4</td>\n",
       "      <td>144.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>M</td>\n",
       "      <td>79.989041</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>MARRIED</td>\n",
       "      <td>UKNOWN</td>\n",
       "      <td>JEWISH</td>\n",
       "      <td>Medicare</td>\n",
       "      <td>EMERGENCY ROOM ADMIT</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.2</td>\n",
       "      <td>30.4</td>\n",
       "      <td>10.7</td>\n",
       "      <td>15.3</td>\n",
       "      <td>128.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>M</td>\n",
       "      <td>76.687671</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>SINGLE</td>\n",
       "      <td>UKNOWN</td>\n",
       "      <td>EPISCOPALIAN</td>\n",
       "      <td>Medicare</td>\n",
       "      <td>CLINIC REFERRAL/PREMATURE</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>26.7</td>\n",
       "      <td>8.7</td>\n",
       "      <td>5.5</td>\n",
       "      <td>135.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>M</td>\n",
       "      <td>50.520548</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>MARRIED</td>\n",
       "      <td>ENGL</td>\n",
       "      <td>PROTESTANT QUAKER</td>\n",
       "      <td>Private</td>\n",
       "      <td>CLINIC REFERRAL/PREMATURE</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.8</td>\n",
       "      <td>20.4</td>\n",
       "      <td>6.5</td>\n",
       "      <td>25.5</td>\n",
       "      <td>230.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>F</td>\n",
       "      <td>66.019178</td>\n",
       "      <td>HISPANIC OR LATINO</td>\n",
       "      <td>SEPARATED</td>\n",
       "      <td>UKNOWN</td>\n",
       "      <td>CATHOLIC</td>\n",
       "      <td>Medicare</td>\n",
       "      <td>EMERGENCY ROOM ADMIT</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.3</td>\n",
       "      <td>30.2</td>\n",
       "      <td>9.7</td>\n",
       "      <td>8.8</td>\n",
       "      <td>145.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   GENDER  ADMIT_AGE           ETHNICITY MARITAL_STATUS LANGUAGE  \\\n",
       "10      M  80.564384               OTHER         SINGLE   UKNOWN   \n",
       "31      M  79.989041               WHITE        MARRIED   UKNOWN   \n",
       "32      M  76.687671               WHITE         SINGLE   UKNOWN   \n",
       "34      M  50.520548               WHITE        MARRIED     ENGL   \n",
       "52      F  66.019178  HISPANIC OR LATINO      SEPARATED   UKNOWN   \n",
       "\n",
       "             RELIGION INSURANCE         ADMISSION_LOCATION  #ADMISSIONS  \\\n",
       "10              OTHER  Medicare       EMERGENCY ROOM ADMIT            1   \n",
       "31             JEWISH  Medicare       EMERGENCY ROOM ADMIT            1   \n",
       "32       EPISCOPALIAN  Medicare  CLINIC REFERRAL/PREMATURE            1   \n",
       "34  PROTESTANT QUAKER   Private  CLINIC REFERRAL/PREMATURE            2   \n",
       "52           CATHOLIC  Medicare       EMERGENCY ROOM ADMIT            1   \n",
       "\n",
       "    DEATH_FLAG  ...  LIVER_FLAG  VALUENUM-8-7-6-5-4-3-2-1  \\\n",
       "10           1  ...           0                       0.7   \n",
       "31           1  ...           0                       2.2   \n",
       "32           1  ...           0                       4.6   \n",
       "34           1  ...           0                       0.7   \n",
       "52           1  ...           0                       3.3   \n",
       "\n",
       "    VALUENUM-8-7-6-5-4-3-2  VALUENUM-8-7-6-5-4-3  VALUENUM-8-7-6-5-4  \\\n",
       "10                     0.0                   0.0                 0.0   \n",
       "31                     0.0                   0.0                 0.0   \n",
       "32                     0.0                   0.0                 0.0   \n",
       "34                     0.0                   0.0                 0.0   \n",
       "52                     0.0                   0.0                 0.0   \n",
       "\n",
       "    VALUENUM-8-7-6-5  VALUENUM-8-7-6  VALUENUM-8-7  VALUENUM-8  VALUENUM  \n",
       "10              13.0            25.5           9.1         3.4     144.0  \n",
       "31              18.2            30.4          10.7        15.3     128.0  \n",
       "32              20.2            26.7           8.7         5.5     135.0  \n",
       "34              13.8            20.4           6.5        25.5     230.0  \n",
       "52              14.3            30.2           9.7         8.8     145.0  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aki_pos['DOA']\n",
    "del aki_pos['DOA']\n",
    "del aki_neg['DOA']\n",
    "\n",
    "del aki_pos['delta']\n",
    "del aki_pos['ITEMID']\n",
    "del aki_pos['ITEMID-3']\n",
    "del aki_pos['ITEMID-3-2']\n",
    "del aki_pos['ITEMID-3-2-1']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "del aki_pos['Unnamed: 0']\n",
    "\n",
    "aki_pos.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Outcome data sets\n",
    "aki_sepsis = pd.Series(aki_pos['KIDNEY_FAILURE_FLAG'])\n",
    "aki_death = pd.Series(aki_pos['DEATH_FLAG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GENDER</th>\n",
       "      <th>ADMIT_AGE</th>\n",
       "      <th>ETHNICITY</th>\n",
       "      <th>MARITAL_STATUS</th>\n",
       "      <th>LANGUAGE</th>\n",
       "      <th>RELIGION</th>\n",
       "      <th>INSURANCE</th>\n",
       "      <th>ADMISSION_LOCATION</th>\n",
       "      <th>#ADMISSIONS</th>\n",
       "      <th>DEATH_FLAG</th>\n",
       "      <th>...</th>\n",
       "      <th>LIVER_FLAG</th>\n",
       "      <th>VALUENUM-8-7-6-5-4-3-2-1</th>\n",
       "      <th>VALUENUM-8-7-6-5-4-3-2</th>\n",
       "      <th>VALUENUM-8-7-6-5-4-3</th>\n",
       "      <th>VALUENUM-8-7-6-5-4</th>\n",
       "      <th>VALUENUM-8-7-6-5</th>\n",
       "      <th>VALUENUM-8-7-6</th>\n",
       "      <th>VALUENUM-8-7</th>\n",
       "      <th>VALUENUM-8</th>\n",
       "      <th>VALUENUM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>M</td>\n",
       "      <td>80.564384</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>SINGLE</td>\n",
       "      <td>UKNOWN</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>Medicare</td>\n",
       "      <td>EMERGENCY ROOM ADMIT</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>25.5</td>\n",
       "      <td>9.1</td>\n",
       "      <td>3.4</td>\n",
       "      <td>144.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>M</td>\n",
       "      <td>79.989041</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>MARRIED</td>\n",
       "      <td>UKNOWN</td>\n",
       "      <td>JEWISH</td>\n",
       "      <td>Medicare</td>\n",
       "      <td>EMERGENCY ROOM ADMIT</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.2</td>\n",
       "      <td>30.4</td>\n",
       "      <td>10.7</td>\n",
       "      <td>15.3</td>\n",
       "      <td>128.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>M</td>\n",
       "      <td>76.687671</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>SINGLE</td>\n",
       "      <td>UKNOWN</td>\n",
       "      <td>EPISCOPALIAN</td>\n",
       "      <td>Medicare</td>\n",
       "      <td>CLINIC REFERRAL/PREMATURE</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>26.7</td>\n",
       "      <td>8.7</td>\n",
       "      <td>5.5</td>\n",
       "      <td>135.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>M</td>\n",
       "      <td>50.520548</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>MARRIED</td>\n",
       "      <td>ENGL</td>\n",
       "      <td>PROTESTANT QUAKER</td>\n",
       "      <td>Private</td>\n",
       "      <td>CLINIC REFERRAL/PREMATURE</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.8</td>\n",
       "      <td>20.4</td>\n",
       "      <td>6.5</td>\n",
       "      <td>25.5</td>\n",
       "      <td>230.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>F</td>\n",
       "      <td>66.019178</td>\n",
       "      <td>HISPANIC OR LATINO</td>\n",
       "      <td>SEPARATED</td>\n",
       "      <td>UKNOWN</td>\n",
       "      <td>CATHOLIC</td>\n",
       "      <td>Medicare</td>\n",
       "      <td>EMERGENCY ROOM ADMIT</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.3</td>\n",
       "      <td>30.2</td>\n",
       "      <td>9.7</td>\n",
       "      <td>8.8</td>\n",
       "      <td>145.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   GENDER  ADMIT_AGE           ETHNICITY MARITAL_STATUS LANGUAGE  \\\n",
       "10      M  80.564384               OTHER         SINGLE   UKNOWN   \n",
       "31      M  79.989041               WHITE        MARRIED   UKNOWN   \n",
       "32      M  76.687671               WHITE         SINGLE   UKNOWN   \n",
       "34      M  50.520548               WHITE        MARRIED     ENGL   \n",
       "52      F  66.019178  HISPANIC OR LATINO      SEPARATED   UKNOWN   \n",
       "\n",
       "             RELIGION INSURANCE         ADMISSION_LOCATION  #ADMISSIONS  \\\n",
       "10              OTHER  Medicare       EMERGENCY ROOM ADMIT            1   \n",
       "31             JEWISH  Medicare       EMERGENCY ROOM ADMIT            1   \n",
       "32       EPISCOPALIAN  Medicare  CLINIC REFERRAL/PREMATURE            1   \n",
       "34  PROTESTANT QUAKER   Private  CLINIC REFERRAL/PREMATURE            2   \n",
       "52           CATHOLIC  Medicare       EMERGENCY ROOM ADMIT            1   \n",
       "\n",
       "    DEATH_FLAG  ...  LIVER_FLAG  VALUENUM-8-7-6-5-4-3-2-1  \\\n",
       "10           1  ...           0                       0.7   \n",
       "31           1  ...           0                       2.2   \n",
       "32           1  ...           0                       4.6   \n",
       "34           1  ...           0                       0.7   \n",
       "52           1  ...           0                       3.3   \n",
       "\n",
       "    VALUENUM-8-7-6-5-4-3-2  VALUENUM-8-7-6-5-4-3  VALUENUM-8-7-6-5-4  \\\n",
       "10                     0.0                   0.0                 0.0   \n",
       "31                     0.0                   0.0                 0.0   \n",
       "32                     0.0                   0.0                 0.0   \n",
       "34                     0.0                   0.0                 0.0   \n",
       "52                     0.0                   0.0                 0.0   \n",
       "\n",
       "    VALUENUM-8-7-6-5  VALUENUM-8-7-6  VALUENUM-8-7  VALUENUM-8  VALUENUM  \n",
       "10              13.0            25.5           9.1         3.4     144.0  \n",
       "31              18.2            30.4          10.7        15.3     128.0  \n",
       "32              20.2            26.7           8.7         5.5     135.0  \n",
       "34              13.8            20.4           6.5        25.5     230.0  \n",
       "52              14.3            30.2           9.7         8.8     145.0  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aki_pos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dummies\n",
    "aki_pos = pd.get_dummies(aki_pos, columns=['GENDER','ETHNICITY','MARITAL_STATUS', 'LANGUAGE', 'RELIGION', 'INSURANCE', 'ADMISSION_LOCATION'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del aki_pos['DEATH_FLAG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ADMIT_AGE</th>\n",
       "      <th>#ADMISSIONS</th>\n",
       "      <th>CANCER_FLAG</th>\n",
       "      <th>KIDNEY_FAILURE_FLAG</th>\n",
       "      <th>HYPO_FLAG</th>\n",
       "      <th>SHOCK_FLAG</th>\n",
       "      <th>ANEMIA_FLAG</th>\n",
       "      <th>HEMO_FLAG</th>\n",
       "      <th>ACID_FLAG</th>\n",
       "      <th>LIVER_FLAG</th>\n",
       "      <th>...</th>\n",
       "      <th>INSURANCE_Medicaid</th>\n",
       "      <th>INSURANCE_Medicare</th>\n",
       "      <th>INSURANCE_Private</th>\n",
       "      <th>INSURANCE_Self Pay</th>\n",
       "      <th>ADMISSION_LOCATION_CLINIC REFERRAL/PREMATURE</th>\n",
       "      <th>ADMISSION_LOCATION_EMERGENCY ROOM ADMIT</th>\n",
       "      <th>ADMISSION_LOCATION_PHYS REFERRAL/NORMAL DELI</th>\n",
       "      <th>ADMISSION_LOCATION_TRANSFER FROM HOSP/EXTRAM</th>\n",
       "      <th>ADMISSION_LOCATION_TRANSFER FROM OTHER HEALT</th>\n",
       "      <th>ADMISSION_LOCATION_TRANSFER FROM SKILLED NUR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>80.564384</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>79.989041</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>76.687671</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>50.520548</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>66.019178</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 114 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ADMIT_AGE  #ADMISSIONS  CANCER_FLAG  KIDNEY_FAILURE_FLAG  HYPO_FLAG  \\\n",
       "10  80.564384            1            1                    0          0   \n",
       "31  79.989041            1            1                    1          0   \n",
       "32  76.687671            1            1                    1          0   \n",
       "34  50.520548            2            1                    0          0   \n",
       "52  66.019178            1            1                    0          0   \n",
       "\n",
       "    SHOCK_FLAG  ANEMIA_FLAG  HEMO_FLAG  ACID_FLAG  LIVER_FLAG  ...  \\\n",
       "10           0            0          0          0           0  ...   \n",
       "31           0            0          0          0           0  ...   \n",
       "32           0            0          0          0           0  ...   \n",
       "34           0            0          0          0           0  ...   \n",
       "52           0            0          0          0           0  ...   \n",
       "\n",
       "    INSURANCE_Medicaid  INSURANCE_Medicare  INSURANCE_Private  \\\n",
       "10                   0                   1                  0   \n",
       "31                   0                   1                  0   \n",
       "32                   0                   1                  0   \n",
       "34                   0                   0                  1   \n",
       "52                   0                   1                  0   \n",
       "\n",
       "    INSURANCE_Self Pay  ADMISSION_LOCATION_CLINIC REFERRAL/PREMATURE  \\\n",
       "10                   0                                             0   \n",
       "31                   0                                             0   \n",
       "32                   0                                             1   \n",
       "34                   0                                             1   \n",
       "52                   0                                             0   \n",
       "\n",
       "    ADMISSION_LOCATION_EMERGENCY ROOM ADMIT  \\\n",
       "10                                        1   \n",
       "31                                        1   \n",
       "32                                        0   \n",
       "34                                        0   \n",
       "52                                        1   \n",
       "\n",
       "    ADMISSION_LOCATION_PHYS REFERRAL/NORMAL DELI  \\\n",
       "10                                             0   \n",
       "31                                             0   \n",
       "32                                             0   \n",
       "34                                             0   \n",
       "52                                             0   \n",
       "\n",
       "    ADMISSION_LOCATION_TRANSFER FROM HOSP/EXTRAM  \\\n",
       "10                                             0   \n",
       "31                                             0   \n",
       "32                                             0   \n",
       "34                                             0   \n",
       "52                                             0   \n",
       "\n",
       "    ADMISSION_LOCATION_TRANSFER FROM OTHER HEALT  \\\n",
       "10                                             0   \n",
       "31                                             0   \n",
       "32                                             0   \n",
       "34                                             0   \n",
       "52                                             0   \n",
       "\n",
       "    ADMISSION_LOCATION_TRANSFER FROM SKILLED NUR  \n",
       "10                                             0  \n",
       "31                                             0  \n",
       "32                                             0  \n",
       "34                                             0  \n",
       "52                                             0  \n",
       "\n",
       "[5 rows x 114 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aki_pos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2159\n"
     ]
    }
   ],
   "source": [
    "print(len(aki_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1998\n",
      "1     161\n",
      "Name: KIDNEY_FAILURE_FLAG, dtype: int64\n",
      "1    1208\n",
      "0     951\n",
      "Name: DEATH_FLAG, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check outcome numbers\n",
    "print(aki_sepsis.value_counts())\n",
    "print(aki_death.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "from sklearn import preprocessing\n",
    "aki_pos1 = preprocessing.minmax_scale(aki_pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test/ train \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(aki_pos1, aki_death, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
    "import matplotlib.pyplot as plt\n",
    "fs1 = SelectKBest(score_func=chi2, k=10)\n",
    "fs2 = SelectKBest(score_func=chi2, k=15)\n",
    "fs3 = SelectKBest(score_func=chi2, k=20)\n",
    "fs4 = SelectKBest(score_func=chi2, k=25)\n",
    "fs5 = SelectKBest(score_func=chi2, k=30)\n",
    "fs6 = SelectKBest(score_func=chi2, k=35)\n",
    "fs7 = SelectKBest(score_func=chi2, k=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs1.fit(X_train, y_train)\n",
    "X_train1 = fs1.transform(X_train)\n",
    "X_test1 = fs1.transform(X_test)\n",
    "\n",
    "fs2.fit(X_train, y_train)\n",
    "X_train2 = fs2.transform(X_train)\n",
    "X_test2 = fs2.transform(X_test)\n",
    "\n",
    "fs3.fit(X_train, y_train)\n",
    "X_train3 = fs3.transform(X_train)\n",
    "X_test3 = fs3.transform(X_test)\n",
    "\n",
    "fs4.fit(X_train, y_train)\n",
    "X_train4 = fs4.transform(X_train)\n",
    "X_test4 = fs4.transform(X_test)\n",
    "\n",
    "fs5.fit(X_train, y_train)\n",
    "X_train5 = fs5.transform(X_train)\n",
    "X_test5 = fs5.transform(X_test)\n",
    "\n",
    "fs6.fit(X_train, y_train)\n",
    "X_train6 = fs6.transform(X_train)\n",
    "X_test6 = fs6.transform(X_test)\n",
    "\n",
    "fs7.fit(X_train, y_train)\n",
    "X_train7 = fs7.transform(X_train)\n",
    "X_test7 = fs7.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "import numpy\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all 7 models for each number of feathers\n",
    "model7 = Sequential()\n",
    "\n",
    "model7.add(Dense(80, input_shape=(40,), kernel_regularizer=regularizers.l2(0.2), activation = 'relu'))\n",
    "model7.add(Dropout(0.3))\n",
    "model7.add(Dense(60, activation = 'relu'))\n",
    "model7.add(Dropout(0.3))\n",
    "model7.add(Dense(40, activation = 'relu'))\n",
    "model7.add(Dropout(0.3))\n",
    "model7.add(Dense(20, activation = 'relu'))\n",
    "model7.add(Dropout(0.3))\n",
    "model7.add(Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile models\n",
    "model7.compile(loss = 'binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1727 samples, validate on 432 samples\n",
      "Epoch 1/250\n",
      "1727/1727 [==============================] - 2s 985us/step - loss: 6.8762 - accuracy: 0.5657 - val_loss: 3.6411 - val_accuracy: 0.5972\n",
      "Epoch 2/250\n",
      "1727/1727 [==============================] - 1s 361us/step - loss: 2.2627 - accuracy: 0.6161 - val_loss: 1.3413 - val_accuracy: 0.6412\n",
      "Epoch 3/250\n",
      "1727/1727 [==============================] - 1s 412us/step - loss: 1.0002 - accuracy: 0.6271 - val_loss: 0.7800 - val_accuracy: 0.6551\n",
      "Epoch 4/250\n",
      "1727/1727 [==============================] - 0s 273us/step - loss: 0.7076 - accuracy: 0.6561 - val_loss: 0.6611 - val_accuracy: 0.6528\n",
      "Epoch 5/250\n",
      "1727/1727 [==============================] - 0s 251us/step - loss: 0.6507 - accuracy: 0.6589 - val_loss: 0.6376 - val_accuracy: 0.6736\n",
      "Epoch 6/250\n",
      "1727/1727 [==============================] - 0s 237us/step - loss: 0.6457 - accuracy: 0.6624 - val_loss: 0.6490 - val_accuracy: 0.6505\n",
      "Epoch 7/250\n",
      "1727/1727 [==============================] - 0s 184us/step - loss: 0.6357 - accuracy: 0.6705 - val_loss: 0.6294 - val_accuracy: 0.6921\n",
      "Epoch 8/250\n",
      "1727/1727 [==============================] - 0s 182us/step - loss: 0.6339 - accuracy: 0.6752 - val_loss: 0.6273 - val_accuracy: 0.6852\n",
      "Epoch 9/250\n",
      "1727/1727 [==============================] - 0s 188us/step - loss: 0.6328 - accuracy: 0.6763 - val_loss: 0.6301 - val_accuracy: 0.6597\n",
      "Epoch 10/250\n",
      "1727/1727 [==============================] - 0s 183us/step - loss: 0.6376 - accuracy: 0.6694 - val_loss: 0.6508 - val_accuracy: 0.6389\n",
      "Epoch 11/250\n",
      "1727/1727 [==============================] - 0s 185us/step - loss: 0.6296 - accuracy: 0.6775 - val_loss: 0.6672 - val_accuracy: 0.6111\n",
      "Epoch 12/250\n",
      "1727/1727 [==============================] - 0s 195us/step - loss: 0.6302 - accuracy: 0.6757 - val_loss: 0.6223 - val_accuracy: 0.7106\n",
      "Epoch 13/250\n",
      "1727/1727 [==============================] - 0s 183us/step - loss: 0.6292 - accuracy: 0.6665 - val_loss: 0.6459 - val_accuracy: 0.6250\n",
      "Epoch 14/250\n",
      "1727/1727 [==============================] - 0s 185us/step - loss: 0.6201 - accuracy: 0.6873 - val_loss: 0.6240 - val_accuracy: 0.6852\n",
      "Epoch 15/250\n",
      "1727/1727 [==============================] - 0s 183us/step - loss: 0.6315 - accuracy: 0.6642 - val_loss: 0.6212 - val_accuracy: 0.6898\n",
      "Epoch 16/250\n",
      "1727/1727 [==============================] - 0s 194us/step - loss: 0.6360 - accuracy: 0.6537 - val_loss: 0.6222 - val_accuracy: 0.7014\n",
      "Epoch 17/250\n",
      "1727/1727 [==============================] - 0s 187us/step - loss: 0.6237 - accuracy: 0.6821 - val_loss: 0.6228 - val_accuracy: 0.6829\n",
      "Epoch 18/250\n",
      "1727/1727 [==============================] - 0s 185us/step - loss: 0.6112 - accuracy: 0.6746 - val_loss: 0.6437 - val_accuracy: 0.6667\n",
      "Epoch 19/250\n",
      "1727/1727 [==============================] - 0s 186us/step - loss: 0.6270 - accuracy: 0.6636 - val_loss: 0.6232 - val_accuracy: 0.6875\n",
      "Epoch 20/250\n",
      "1727/1727 [==============================] - 0s 184us/step - loss: 0.6159 - accuracy: 0.6711 - val_loss: 0.6249 - val_accuracy: 0.6829\n",
      "Epoch 21/250\n",
      "1727/1727 [==============================] - 0s 184us/step - loss: 0.6106 - accuracy: 0.6792 - val_loss: 0.6322 - val_accuracy: 0.6667\n",
      "Epoch 22/250\n",
      "1727/1727 [==============================] - 0s 183us/step - loss: 0.6165 - accuracy: 0.6705 - val_loss: 0.6267 - val_accuracy: 0.6806\n",
      "Epoch 23/250\n",
      "1727/1727 [==============================] - 0s 185us/step - loss: 0.6130 - accuracy: 0.6781 - val_loss: 0.6223 - val_accuracy: 0.6829\n",
      "Epoch 24/250\n",
      "1727/1727 [==============================] - 0s 183us/step - loss: 0.6145 - accuracy: 0.6856 - val_loss: 0.6277 - val_accuracy: 0.6690\n",
      "Epoch 25/250\n",
      "1727/1727 [==============================] - 0s 193us/step - loss: 0.6122 - accuracy: 0.6809 - val_loss: 0.6346 - val_accuracy: 0.6620\n",
      "Epoch 26/250\n",
      "1727/1727 [==============================] - 0s 195us/step - loss: 0.6116 - accuracy: 0.6804 - val_loss: 0.6251 - val_accuracy: 0.6782\n",
      "Epoch 27/250\n",
      "1727/1727 [==============================] - 0s 186us/step - loss: 0.6170 - accuracy: 0.6798 - val_loss: 0.6171 - val_accuracy: 0.6968\n",
      "Epoch 28/250\n",
      "1727/1727 [==============================] - 0s 202us/step - loss: 0.6082 - accuracy: 0.6833 - val_loss: 0.6482 - val_accuracy: 0.6319\n",
      "Epoch 29/250\n",
      "1727/1727 [==============================] - 0s 187us/step - loss: 0.6084 - accuracy: 0.6908 - val_loss: 0.6669 - val_accuracy: 0.6366\n",
      "Epoch 30/250\n",
      "1727/1727 [==============================] - 0s 220us/step - loss: 0.6190 - accuracy: 0.6838 - val_loss: 0.6198 - val_accuracy: 0.6829\n",
      "Epoch 31/250\n",
      "1727/1727 [==============================] - 1s 295us/step - loss: 0.6108 - accuracy: 0.6827 - val_loss: 0.6264 - val_accuracy: 0.6620\n",
      "Epoch 32/250\n",
      "1727/1727 [==============================] - 0s 264us/step - loss: 0.6078 - accuracy: 0.6862 - val_loss: 0.6132 - val_accuracy: 0.6944\n",
      "Epoch 33/250\n",
      "1727/1727 [==============================] - 0s 246us/step - loss: 0.6095 - accuracy: 0.6815 - val_loss: 0.6320 - val_accuracy: 0.6667\n",
      "Epoch 34/250\n",
      "1727/1727 [==============================] - 0s 278us/step - loss: 0.6059 - accuracy: 0.6815 - val_loss: 0.6171 - val_accuracy: 0.7037\n",
      "Epoch 35/250\n",
      "1727/1727 [==============================] - 0s 276us/step - loss: 0.6120 - accuracy: 0.6781 - val_loss: 0.6371 - val_accuracy: 0.6597\n",
      "Epoch 36/250\n",
      "1727/1727 [==============================] - 0s 229us/step - loss: 0.6098 - accuracy: 0.6711 - val_loss: 0.6233 - val_accuracy: 0.6620\n",
      "Epoch 37/250\n",
      "1727/1727 [==============================] - 0s 267us/step - loss: 0.5980 - accuracy: 0.6844 - val_loss: 0.6137 - val_accuracy: 0.7083\n",
      "Epoch 38/250\n",
      "1727/1727 [==============================] - 0s 254us/step - loss: 0.5960 - accuracy: 0.6948 - val_loss: 0.6188 - val_accuracy: 0.7014\n",
      "Epoch 39/250\n",
      "1727/1727 [==============================] - 0s 234us/step - loss: 0.5965 - accuracy: 0.6891 - val_loss: 0.6117 - val_accuracy: 0.6898\n",
      "Epoch 40/250\n",
      "1727/1727 [==============================] - 0s 226us/step - loss: 0.6000 - accuracy: 0.6833 - val_loss: 0.6142 - val_accuracy: 0.7083\n",
      "Epoch 41/250\n",
      "1727/1727 [==============================] - 0s 255us/step - loss: 0.5941 - accuracy: 0.6920 - val_loss: 0.6254 - val_accuracy: 0.6898\n",
      "Epoch 42/250\n",
      "1727/1727 [==============================] - 0s 222us/step - loss: 0.5980 - accuracy: 0.6775 - val_loss: 0.6221 - val_accuracy: 0.6898\n",
      "Epoch 43/250\n",
      "1727/1727 [==============================] - 0s 257us/step - loss: 0.6004 - accuracy: 0.6862 - val_loss: 0.6326 - val_accuracy: 0.6736\n",
      "Epoch 44/250\n",
      "1727/1727 [==============================] - 0s 230us/step - loss: 0.5999 - accuracy: 0.6902 - val_loss: 0.6137 - val_accuracy: 0.6667\n",
      "Epoch 45/250\n",
      "1727/1727 [==============================] - 0s 229us/step - loss: 0.6022 - accuracy: 0.6838 - val_loss: 0.6150 - val_accuracy: 0.6944\n",
      "Epoch 46/250\n",
      "1727/1727 [==============================] - 0s 226us/step - loss: 0.5984 - accuracy: 0.6948 - val_loss: 0.6255 - val_accuracy: 0.6759\n",
      "Epoch 47/250\n",
      "1727/1727 [==============================] - 0s 259us/step - loss: 0.6015 - accuracy: 0.6862 - val_loss: 0.6247 - val_accuracy: 0.6806\n",
      "Epoch 48/250\n",
      "1727/1727 [==============================] - 0s 222us/step - loss: 0.6039 - accuracy: 0.6821 - val_loss: 0.6276 - val_accuracy: 0.6505\n",
      "Epoch 49/250\n",
      "1727/1727 [==============================] - 0s 220us/step - loss: 0.6000 - accuracy: 0.6885 - val_loss: 0.6246 - val_accuracy: 0.6736\n",
      "Epoch 50/250\n",
      "1727/1727 [==============================] - 0s 231us/step - loss: 0.5942 - accuracy: 0.6989 - val_loss: 0.6257 - val_accuracy: 0.6667\n",
      "Epoch 51/250\n",
      "1727/1727 [==============================] - 0s 237us/step - loss: 0.5977 - accuracy: 0.6879 - val_loss: 0.6261 - val_accuracy: 0.6505\n",
      "Epoch 52/250\n",
      "1727/1727 [==============================] - 0s 230us/step - loss: 0.5926 - accuracy: 0.6885 - val_loss: 0.6383 - val_accuracy: 0.6759\n",
      "Epoch 53/250\n",
      "1727/1727 [==============================] - 0s 250us/step - loss: 0.5999 - accuracy: 0.6740 - val_loss: 0.6461 - val_accuracy: 0.6782\n",
      "Epoch 54/250\n",
      "1727/1727 [==============================] - 0s 284us/step - loss: 0.5967 - accuracy: 0.6809 - val_loss: 0.6241 - val_accuracy: 0.6782\n",
      "Epoch 55/250\n",
      "1727/1727 [==============================] - 0s 199us/step - loss: 0.6004 - accuracy: 0.6740 - val_loss: 0.6078 - val_accuracy: 0.6852\n",
      "Epoch 56/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1727/1727 [==============================] - 0s 212us/step - loss: 0.5935 - accuracy: 0.6873 - val_loss: 0.6280 - val_accuracy: 0.6713\n",
      "Epoch 57/250\n",
      "1727/1727 [==============================] - 0s 202us/step - loss: 0.5865 - accuracy: 0.6983 - val_loss: 0.6096 - val_accuracy: 0.7106\n",
      "Epoch 58/250\n",
      "1727/1727 [==============================] - 0s 203us/step - loss: 0.5956 - accuracy: 0.6827 - val_loss: 0.6036 - val_accuracy: 0.7106\n",
      "Epoch 59/250\n",
      "1727/1727 [==============================] - 0s 200us/step - loss: 0.6084 - accuracy: 0.6827 - val_loss: 0.6364 - val_accuracy: 0.6713\n",
      "Epoch 60/250\n",
      "1727/1727 [==============================] - 0s 218us/step - loss: 0.5961 - accuracy: 0.6920 - val_loss: 0.6387 - val_accuracy: 0.6273\n",
      "Epoch 61/250\n",
      "1727/1727 [==============================] - 0s 278us/step - loss: 0.5983 - accuracy: 0.6752 - val_loss: 0.6331 - val_accuracy: 0.6875\n",
      "Epoch 62/250\n",
      "1727/1727 [==============================] - 0s 203us/step - loss: 0.5963 - accuracy: 0.6896 - val_loss: 0.6243 - val_accuracy: 0.6690\n",
      "Epoch 63/250\n",
      "1727/1727 [==============================] - 0s 201us/step - loss: 0.5971 - accuracy: 0.6914 - val_loss: 0.6182 - val_accuracy: 0.6852\n",
      "Epoch 64/250\n",
      "1727/1727 [==============================] - 0s 200us/step - loss: 0.6004 - accuracy: 0.6891 - val_loss: 0.6223 - val_accuracy: 0.6806\n",
      "Epoch 65/250\n",
      "1727/1727 [==============================] - 0s 206us/step - loss: 0.5916 - accuracy: 0.7035 - val_loss: 0.6196 - val_accuracy: 0.6898\n",
      "Epoch 66/250\n",
      "1727/1727 [==============================] - 0s 203us/step - loss: 0.5957 - accuracy: 0.6920 - val_loss: 0.6343 - val_accuracy: 0.6597\n",
      "Epoch 67/250\n",
      "1727/1727 [==============================] - 1s 290us/step - loss: 0.5911 - accuracy: 0.6885 - val_loss: 0.6116 - val_accuracy: 0.6852\n",
      "Epoch 68/250\n",
      "1727/1727 [==============================] - 0s 205us/step - loss: 0.6003 - accuracy: 0.6867 - val_loss: 0.6122 - val_accuracy: 0.6806\n",
      "Epoch 69/250\n",
      "1727/1727 [==============================] - 0s 201us/step - loss: 0.5858 - accuracy: 0.6954 - val_loss: 0.6232 - val_accuracy: 0.6921\n",
      "Epoch 70/250\n",
      "1727/1727 [==============================] - 0s 213us/step - loss: 0.6016 - accuracy: 0.6804 - val_loss: 0.6500 - val_accuracy: 0.6435\n",
      "Epoch 71/250\n",
      "1727/1727 [==============================] - 0s 218us/step - loss: 0.5980 - accuracy: 0.6850 - val_loss: 0.6186 - val_accuracy: 0.6968\n",
      "Epoch 72/250\n",
      "1727/1727 [==============================] - 0s 216us/step - loss: 0.5915 - accuracy: 0.6833 - val_loss: 0.6206 - val_accuracy: 0.6921\n",
      "Epoch 73/250\n",
      "1727/1727 [==============================] - 0s 216us/step - loss: 0.5959 - accuracy: 0.6844 - val_loss: 0.6201 - val_accuracy: 0.6806\n",
      "Epoch 74/250\n",
      "1727/1727 [==============================] - 0s 214us/step - loss: 0.5879 - accuracy: 0.6943 - val_loss: 0.6236 - val_accuracy: 0.6921\n",
      "Epoch 75/250\n",
      "1727/1727 [==============================] - 0s 216us/step - loss: 0.5862 - accuracy: 0.6862 - val_loss: 0.6205 - val_accuracy: 0.6875\n",
      "Epoch 76/250\n",
      "1727/1727 [==============================] - 0s 206us/step - loss: 0.5839 - accuracy: 0.6943 - val_loss: 0.6150 - val_accuracy: 0.6944\n",
      "Epoch 77/250\n",
      "1727/1727 [==============================] - 0s 203us/step - loss: 0.5889 - accuracy: 0.6943 - val_loss: 0.6690 - val_accuracy: 0.6551\n",
      "Epoch 78/250\n",
      "1727/1727 [==============================] - 0s 199us/step - loss: 0.5971 - accuracy: 0.6891 - val_loss: 0.6163 - val_accuracy: 0.6944\n",
      "Epoch 79/250\n",
      "1727/1727 [==============================] - 0s 205us/step - loss: 0.5944 - accuracy: 0.6867 - val_loss: 0.6055 - val_accuracy: 0.6921\n",
      "Epoch 80/250\n",
      "1727/1727 [==============================] - 0s 202us/step - loss: 0.5961 - accuracy: 0.6902 - val_loss: 0.6030 - val_accuracy: 0.7014\n",
      "Epoch 81/250\n",
      "1727/1727 [==============================] - 0s 220us/step - loss: 0.5954 - accuracy: 0.6856 - val_loss: 0.6293 - val_accuracy: 0.6736\n",
      "Epoch 82/250\n",
      "1727/1727 [==============================] - 0s 198us/step - loss: 0.5991 - accuracy: 0.6862 - val_loss: 0.6195 - val_accuracy: 0.7106\n",
      "Epoch 83/250\n",
      "1727/1727 [==============================] - 0s 219us/step - loss: 0.5926 - accuracy: 0.6885 - val_loss: 0.6287 - val_accuracy: 0.6921\n",
      "Epoch 84/250\n",
      "1727/1727 [==============================] - 0s 206us/step - loss: 0.5942 - accuracy: 0.6775 - val_loss: 0.6131 - val_accuracy: 0.6898\n",
      "Epoch 85/250\n",
      "1727/1727 [==============================] - 0s 222us/step - loss: 0.5954 - accuracy: 0.6833 - val_loss: 0.6080 - val_accuracy: 0.6782\n",
      "Epoch 86/250\n",
      "1727/1727 [==============================] - 0s 206us/step - loss: 0.5870 - accuracy: 0.6885 - val_loss: 0.6173 - val_accuracy: 0.6852\n",
      "Epoch 87/250\n",
      "1727/1727 [==============================] - 0s 214us/step - loss: 0.5893 - accuracy: 0.6844 - val_loss: 0.6135 - val_accuracy: 0.7014\n",
      "Epoch 88/250\n",
      "1727/1727 [==============================] - 0s 208us/step - loss: 0.5935 - accuracy: 0.6896 - val_loss: 0.6232 - val_accuracy: 0.6319\n",
      "Epoch 89/250\n",
      "1727/1727 [==============================] - 0s 213us/step - loss: 0.5916 - accuracy: 0.6914 - val_loss: 0.6221 - val_accuracy: 0.6574\n",
      "Epoch 90/250\n",
      "1727/1727 [==============================] - 0s 204us/step - loss: 0.5977 - accuracy: 0.6943 - val_loss: 0.6169 - val_accuracy: 0.7106\n",
      "Epoch 91/250\n",
      "1727/1727 [==============================] - 0s 207us/step - loss: 0.5976 - accuracy: 0.6891 - val_loss: 0.6195 - val_accuracy: 0.6875\n",
      "Epoch 92/250\n",
      "1727/1727 [==============================] - 0s 230us/step - loss: 0.5955 - accuracy: 0.6757 - val_loss: 0.6304 - val_accuracy: 0.6644\n",
      "Epoch 93/250\n",
      "1727/1727 [==============================] - 0s 207us/step - loss: 0.5836 - accuracy: 0.6920 - val_loss: 0.6084 - val_accuracy: 0.6852\n",
      "Epoch 94/250\n",
      "1727/1727 [==============================] - 0s 209us/step - loss: 0.5911 - accuracy: 0.6914 - val_loss: 0.6233 - val_accuracy: 0.6898\n",
      "Epoch 95/250\n",
      "1727/1727 [==============================] - 0s 216us/step - loss: 0.5906 - accuracy: 0.6972 - val_loss: 0.6159 - val_accuracy: 0.7014\n",
      "Epoch 96/250\n",
      "1727/1727 [==============================] - 0s 206us/step - loss: 0.5910 - accuracy: 0.6948 - val_loss: 0.6297 - val_accuracy: 0.6481\n",
      "Epoch 97/250\n",
      "1727/1727 [==============================] - 0s 203us/step - loss: 0.6086 - accuracy: 0.6786 - val_loss: 0.6254 - val_accuracy: 0.6690\n",
      "Epoch 98/250\n",
      "1727/1727 [==============================] - 0s 230us/step - loss: 0.5949 - accuracy: 0.6833 - val_loss: 0.6170 - val_accuracy: 0.6944\n",
      "Epoch 99/250\n",
      "1727/1727 [==============================] - 0s 206us/step - loss: 0.5864 - accuracy: 0.6960 - val_loss: 0.6168 - val_accuracy: 0.6875\n",
      "Epoch 100/250\n",
      "1727/1727 [==============================] - 0s 208us/step - loss: 0.5922 - accuracy: 0.6966 - val_loss: 0.6159 - val_accuracy: 0.6968\n",
      "Epoch 101/250\n",
      "1727/1727 [==============================] - 0s 216us/step - loss: 0.5824 - accuracy: 0.7001 - val_loss: 0.6173 - val_accuracy: 0.7060\n",
      "Epoch 102/250\n",
      "1727/1727 [==============================] - 0s 211us/step - loss: 0.5963 - accuracy: 0.6983 - val_loss: 0.6130 - val_accuracy: 0.6852\n",
      "Epoch 103/250\n",
      "1727/1727 [==============================] - 0s 205us/step - loss: 0.5973 - accuracy: 0.6948 - val_loss: 0.6141 - val_accuracy: 0.6921\n",
      "Epoch 104/250\n",
      "1727/1727 [==============================] - 0s 211us/step - loss: 0.5988 - accuracy: 0.6856 - val_loss: 0.6011 - val_accuracy: 0.7130\n",
      "Epoch 105/250\n",
      "1727/1727 [==============================] - 0s 205us/step - loss: 0.5919 - accuracy: 0.6804 - val_loss: 0.6076 - val_accuracy: 0.6944\n",
      "Epoch 106/250\n",
      "1727/1727 [==============================] - 0s 211us/step - loss: 0.5986 - accuracy: 0.6827 - val_loss: 0.6139 - val_accuracy: 0.6852\n",
      "Epoch 107/250\n",
      "1727/1727 [==============================] - 0s 211us/step - loss: 0.5973 - accuracy: 0.6809 - val_loss: 0.6085 - val_accuracy: 0.6944\n",
      "Epoch 108/250\n",
      "1727/1727 [==============================] - 0s 218us/step - loss: 0.5941 - accuracy: 0.6873 - val_loss: 0.6219 - val_accuracy: 0.6690\n",
      "Epoch 109/250\n",
      "1727/1727 [==============================] - 0s 204us/step - loss: 0.5864 - accuracy: 0.6867 - val_loss: 0.6122 - val_accuracy: 0.6759\n",
      "Epoch 110/250\n",
      "1727/1727 [==============================] - 0s 208us/step - loss: 0.6011 - accuracy: 0.6827 - val_loss: 0.6380 - val_accuracy: 0.6204\n",
      "Epoch 111/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1727/1727 [==============================] - 0s 201us/step - loss: 0.5882 - accuracy: 0.6925 - val_loss: 0.6172 - val_accuracy: 0.6829\n",
      "Epoch 112/250\n",
      "1727/1727 [==============================] - 0s 201us/step - loss: 0.5920 - accuracy: 0.6815 - val_loss: 0.6070 - val_accuracy: 0.6968\n",
      "Epoch 113/250\n",
      "1727/1727 [==============================] - 0s 212us/step - loss: 0.5899 - accuracy: 0.6850 - val_loss: 0.6143 - val_accuracy: 0.6921\n",
      "Epoch 114/250\n",
      "1727/1727 [==============================] - 1s 293us/step - loss: 0.5922 - accuracy: 0.6891 - val_loss: 0.6054 - val_accuracy: 0.6898\n",
      "Epoch 115/250\n",
      "1727/1727 [==============================] - 0s 272us/step - loss: 0.5941 - accuracy: 0.6821 - val_loss: 0.6106 - val_accuracy: 0.6991\n",
      "Epoch 116/250\n",
      "1727/1727 [==============================] - 0s 259us/step - loss: 0.5908 - accuracy: 0.6937 - val_loss: 0.5983 - val_accuracy: 0.6898\n",
      "Epoch 117/250\n",
      "1727/1727 [==============================] - 1s 292us/step - loss: 0.6006 - accuracy: 0.6896 - val_loss: 0.6240 - val_accuracy: 0.6528\n",
      "Epoch 118/250\n",
      "1727/1727 [==============================] - 0s 255us/step - loss: 0.5939 - accuracy: 0.6862 - val_loss: 0.6155 - val_accuracy: 0.6759\n",
      "Epoch 119/250\n",
      "1727/1727 [==============================] - 0s 228us/step - loss: 0.5965 - accuracy: 0.6873 - val_loss: 0.6045 - val_accuracy: 0.6829\n",
      "Epoch 120/250\n",
      "1727/1727 [==============================] - 0s 249us/step - loss: 0.5986 - accuracy: 0.6896 - val_loss: 0.6105 - val_accuracy: 0.6782\n",
      "Epoch 121/250\n",
      "1727/1727 [==============================] - 0s 240us/step - loss: 0.5821 - accuracy: 0.6995 - val_loss: 0.6470 - val_accuracy: 0.6435\n",
      "Epoch 122/250\n",
      "1727/1727 [==============================] - 0s 241us/step - loss: 0.5792 - accuracy: 0.6954 - val_loss: 0.6173 - val_accuracy: 0.6968\n",
      "Epoch 123/250\n",
      "1727/1727 [==============================] - 0s 231us/step - loss: 0.6027 - accuracy: 0.6891 - val_loss: 0.6096 - val_accuracy: 0.6875\n",
      "Epoch 124/250\n",
      "1727/1727 [==============================] - 0s 226us/step - loss: 0.5951 - accuracy: 0.6948 - val_loss: 0.6073 - val_accuracy: 0.6898\n",
      "Epoch 125/250\n",
      "1727/1727 [==============================] - 0s 241us/step - loss: 0.5916 - accuracy: 0.6891 - val_loss: 0.6226 - val_accuracy: 0.6852\n",
      "Epoch 126/250\n",
      "1727/1727 [==============================] - 0s 227us/step - loss: 0.5982 - accuracy: 0.6960 - val_loss: 0.6065 - val_accuracy: 0.6944\n",
      "Epoch 127/250\n",
      "1727/1727 [==============================] - 0s 239us/step - loss: 0.5942 - accuracy: 0.6856 - val_loss: 0.6142 - val_accuracy: 0.6806\n",
      "Epoch 128/250\n",
      "1727/1727 [==============================] - 0s 225us/step - loss: 0.5915 - accuracy: 0.6972 - val_loss: 0.6250 - val_accuracy: 0.6574\n",
      "Epoch 129/250\n",
      "1727/1727 [==============================] - 0s 251us/step - loss: 0.5838 - accuracy: 0.7024 - val_loss: 0.6133 - val_accuracy: 0.6875\n",
      "Epoch 130/250\n",
      "1727/1727 [==============================] - 0s 225us/step - loss: 0.5846 - accuracy: 0.6943 - val_loss: 0.6105 - val_accuracy: 0.6690\n",
      "Epoch 131/250\n",
      "1727/1727 [==============================] - 0s 231us/step - loss: 0.5928 - accuracy: 0.6862 - val_loss: 0.6062 - val_accuracy: 0.6944\n",
      "Epoch 132/250\n",
      "1727/1727 [==============================] - 0s 224us/step - loss: 0.5975 - accuracy: 0.6943 - val_loss: 0.6008 - val_accuracy: 0.7060\n",
      "Epoch 133/250\n",
      "1727/1727 [==============================] - 0s 215us/step - loss: 0.5957 - accuracy: 0.6838 - val_loss: 0.6120 - val_accuracy: 0.6944\n",
      "Epoch 134/250\n",
      "1727/1727 [==============================] - 0s 217us/step - loss: 0.5934 - accuracy: 0.7006 - val_loss: 0.6173 - val_accuracy: 0.6574\n",
      "Epoch 135/250\n",
      "1727/1727 [==============================] - 0s 208us/step - loss: 0.5870 - accuracy: 0.6920 - val_loss: 0.6230 - val_accuracy: 0.6667\n",
      "Epoch 136/250\n",
      "1727/1727 [==============================] - 0s 222us/step - loss: 0.6006 - accuracy: 0.6862 - val_loss: 0.6557 - val_accuracy: 0.6852\n",
      "Epoch 137/250\n",
      "1727/1727 [==============================] - 0s 222us/step - loss: 0.5913 - accuracy: 0.6995 - val_loss: 0.6314 - val_accuracy: 0.7014\n",
      "Epoch 138/250\n",
      "1727/1727 [==============================] - 0s 222us/step - loss: 0.6053 - accuracy: 0.6815 - val_loss: 0.6109 - val_accuracy: 0.7014\n",
      "Epoch 139/250\n",
      "1727/1727 [==============================] - 0s 217us/step - loss: 0.5947 - accuracy: 0.6838 - val_loss: 0.6096 - val_accuracy: 0.6968\n",
      "Epoch 140/250\n",
      "1727/1727 [==============================] - 0s 208us/step - loss: 0.5867 - accuracy: 0.6896 - val_loss: 0.6281 - val_accuracy: 0.6921\n",
      "Epoch 141/250\n",
      "1727/1727 [==============================] - 0s 217us/step - loss: 0.5929 - accuracy: 0.6896 - val_loss: 0.6289 - val_accuracy: 0.6736\n",
      "Epoch 142/250\n",
      "1727/1727 [==============================] - 0s 233us/step - loss: 0.5890 - accuracy: 0.6867 - val_loss: 0.6289 - val_accuracy: 0.6944\n",
      "Epoch 143/250\n",
      "1727/1727 [==============================] - 0s 220us/step - loss: 0.5907 - accuracy: 0.6856 - val_loss: 0.6407 - val_accuracy: 0.6875\n",
      "Epoch 144/250\n",
      "1727/1727 [==============================] - 0s 218us/step - loss: 0.5953 - accuracy: 0.6833 - val_loss: 0.6128 - val_accuracy: 0.6968\n",
      "Epoch 145/250\n",
      "1727/1727 [==============================] - 0s 288us/step - loss: 0.5846 - accuracy: 0.6937 - val_loss: 0.6268 - val_accuracy: 0.6574\n",
      "Epoch 146/250\n",
      "1727/1727 [==============================] - 0s 222us/step - loss: 0.5954 - accuracy: 0.6960 - val_loss: 0.6054 - val_accuracy: 0.6944\n",
      "Epoch 147/250\n",
      "1727/1727 [==============================] - 0s 225us/step - loss: 0.5896 - accuracy: 0.6989 - val_loss: 0.6256 - val_accuracy: 0.6782\n",
      "Epoch 148/250\n",
      "1727/1727 [==============================] - 0s 224us/step - loss: 0.5944 - accuracy: 0.6902 - val_loss: 0.6224 - val_accuracy: 0.6875\n",
      "Epoch 149/250\n",
      "1727/1727 [==============================] - 0s 205us/step - loss: 0.5845 - accuracy: 0.6983 - val_loss: 0.6192 - val_accuracy: 0.6921\n",
      "Epoch 150/250\n",
      "1727/1727 [==============================] - 0s 205us/step - loss: 0.5883 - accuracy: 0.6943 - val_loss: 0.6211 - val_accuracy: 0.6852\n",
      "Epoch 151/250\n",
      "1727/1727 [==============================] - 0s 204us/step - loss: 0.5868 - accuracy: 0.6885 - val_loss: 0.6183 - val_accuracy: 0.6898\n",
      "Epoch 152/250\n",
      "1727/1727 [==============================] - 0s 218us/step - loss: 0.5853 - accuracy: 0.6977 - val_loss: 0.6211 - val_accuracy: 0.6968\n",
      "Epoch 153/250\n",
      "1727/1727 [==============================] - 0s 206us/step - loss: 0.5828 - accuracy: 0.7012 - val_loss: 0.6208 - val_accuracy: 0.6782\n",
      "Epoch 154/250\n",
      "1727/1727 [==============================] - 0s 214us/step - loss: 0.5767 - accuracy: 0.6983 - val_loss: 0.6098 - val_accuracy: 0.6898\n",
      "Epoch 155/250\n",
      "1727/1727 [==============================] - 0s 212us/step - loss: 0.5920 - accuracy: 0.7012 - val_loss: 0.6136 - val_accuracy: 0.7014\n",
      "Epoch 156/250\n",
      "1727/1727 [==============================] - 0s 206us/step - loss: 0.5940 - accuracy: 0.6873 - val_loss: 0.6213 - val_accuracy: 0.6667\n",
      "Epoch 157/250\n",
      "1727/1727 [==============================] - 0s 207us/step - loss: 0.5875 - accuracy: 0.6879 - val_loss: 0.6329 - val_accuracy: 0.6991\n",
      "Epoch 158/250\n",
      "1727/1727 [==============================] - 0s 261us/step - loss: 0.5954 - accuracy: 0.6931 - val_loss: 0.6197 - val_accuracy: 0.6690\n",
      "Epoch 159/250\n",
      "1727/1727 [==============================] - 0s 214us/step - loss: 0.5804 - accuracy: 0.6983 - val_loss: 0.6255 - val_accuracy: 0.6667\n",
      "Epoch 160/250\n",
      "1727/1727 [==============================] - 0s 236us/step - loss: 0.5915 - accuracy: 0.6948 - val_loss: 0.6298 - val_accuracy: 0.6713\n",
      "Epoch 161/250\n",
      "1727/1727 [==============================] - 0s 218us/step - loss: 0.5902 - accuracy: 0.6833 - val_loss: 0.6427 - val_accuracy: 0.6620\n",
      "Epoch 162/250\n",
      "1727/1727 [==============================] - 0s 216us/step - loss: 0.5882 - accuracy: 0.6948 - val_loss: 0.6243 - val_accuracy: 0.6806\n",
      "Epoch 163/250\n",
      "1727/1727 [==============================] - 0s 218us/step - loss: 0.5832 - accuracy: 0.7012 - val_loss: 0.6187 - val_accuracy: 0.7014\n",
      "Epoch 164/250\n",
      "1727/1727 [==============================] - 0s 220us/step - loss: 0.5902 - accuracy: 0.6948 - val_loss: 0.6202 - val_accuracy: 0.6806\n",
      "Epoch 165/250\n",
      "1727/1727 [==============================] - 0s 219us/step - loss: 0.5943 - accuracy: 0.7006 - val_loss: 0.6388 - val_accuracy: 0.6713\n",
      "Epoch 166/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1727/1727 [==============================] - 0s 204us/step - loss: 0.5873 - accuracy: 0.7035 - val_loss: 0.6305 - val_accuracy: 0.6806\n",
      "Epoch 167/250\n",
      "1727/1727 [==============================] - 0s 206us/step - loss: 0.5876 - accuracy: 0.6856 - val_loss: 0.6232 - val_accuracy: 0.7037\n",
      "Epoch 168/250\n",
      "1727/1727 [==============================] - 0s 204us/step - loss: 0.5877 - accuracy: 0.6995 - val_loss: 0.6224 - val_accuracy: 0.6736\n",
      "Epoch 169/250\n",
      "1727/1727 [==============================] - 0s 208us/step - loss: 0.5898 - accuracy: 0.6925 - val_loss: 0.6309 - val_accuracy: 0.6435\n",
      "Epoch 170/250\n",
      "1727/1727 [==============================] - 0s 205us/step - loss: 0.5732 - accuracy: 0.6983 - val_loss: 0.6259 - val_accuracy: 0.6968\n",
      "Epoch 171/250\n",
      "1727/1727 [==============================] - 0s 202us/step - loss: 0.5891 - accuracy: 0.6954 - val_loss: 0.6809 - val_accuracy: 0.6227\n",
      "Epoch 172/250\n",
      "1727/1727 [==============================] - 0s 199us/step - loss: 0.5959 - accuracy: 0.6902 - val_loss: 0.6191 - val_accuracy: 0.6921\n",
      "Epoch 173/250\n",
      "1727/1727 [==============================] - 0s 202us/step - loss: 0.5926 - accuracy: 0.6943 - val_loss: 0.6200 - val_accuracy: 0.6991\n",
      "Epoch 174/250\n",
      "1727/1727 [==============================] - 0s 205us/step - loss: 0.5859 - accuracy: 0.7041 - val_loss: 0.6318 - val_accuracy: 0.6898\n",
      "Epoch 175/250\n",
      "1727/1727 [==============================] - 0s 204us/step - loss: 0.5931 - accuracy: 0.6914 - val_loss: 0.6235 - val_accuracy: 0.6944\n",
      "Epoch 176/250\n",
      "1727/1727 [==============================] - 0s 207us/step - loss: 0.5962 - accuracy: 0.6838 - val_loss: 0.6352 - val_accuracy: 0.6759\n",
      "Epoch 177/250\n",
      "1727/1727 [==============================] - 0s 214us/step - loss: 0.6036 - accuracy: 0.6757 - val_loss: 0.6127 - val_accuracy: 0.6759\n",
      "Epoch 178/250\n",
      "1727/1727 [==============================] - 0s 215us/step - loss: 0.5964 - accuracy: 0.6960 - val_loss: 0.6178 - val_accuracy: 0.7037\n",
      "Epoch 179/250\n",
      "1727/1727 [==============================] - 0s 208us/step - loss: 0.5884 - accuracy: 0.6948 - val_loss: 0.6286 - val_accuracy: 0.6759\n",
      "Epoch 180/250\n",
      "1727/1727 [==============================] - 0s 220us/step - loss: 0.5930 - accuracy: 0.6862 - val_loss: 0.6170 - val_accuracy: 0.6944\n",
      "Epoch 181/250\n",
      "1727/1727 [==============================] - 0s 214us/step - loss: 0.5884 - accuracy: 0.6972 - val_loss: 0.6105 - val_accuracy: 0.6968\n",
      "Epoch 182/250\n",
      "1727/1727 [==============================] - 0s 205us/step - loss: 0.5793 - accuracy: 0.6995 - val_loss: 0.6179 - val_accuracy: 0.7014\n",
      "Epoch 183/250\n",
      "1727/1727 [==============================] - 0s 216us/step - loss: 0.5935 - accuracy: 0.6867 - val_loss: 0.6272 - val_accuracy: 0.6690\n",
      "Epoch 184/250\n",
      "1727/1727 [==============================] - 0s 208us/step - loss: 0.5848 - accuracy: 0.7001 - val_loss: 0.6332 - val_accuracy: 0.6968\n",
      "Epoch 185/250\n",
      "1727/1727 [==============================] - 0s 254us/step - loss: 0.5872 - accuracy: 0.6879 - val_loss: 0.6320 - val_accuracy: 0.7083\n",
      "Epoch 186/250\n",
      "1727/1727 [==============================] - 0s 217us/step - loss: 0.5825 - accuracy: 0.7087 - val_loss: 0.6438 - val_accuracy: 0.6829\n",
      "Epoch 187/250\n",
      "1727/1727 [==============================] - 0s 204us/step - loss: 0.5965 - accuracy: 0.6891 - val_loss: 0.6177 - val_accuracy: 0.6574\n",
      "Epoch 188/250\n",
      "1727/1727 [==============================] - 0s 212us/step - loss: 0.5900 - accuracy: 0.6937 - val_loss: 0.6196 - val_accuracy: 0.6829\n",
      "Epoch 189/250\n",
      "1727/1727 [==============================] - 0s 227us/step - loss: 0.5867 - accuracy: 0.6844 - val_loss: 0.6337 - val_accuracy: 0.6944\n",
      "Epoch 190/250\n",
      "1727/1727 [==============================] - 0s 204us/step - loss: 0.5914 - accuracy: 0.7058 - val_loss: 0.6133 - val_accuracy: 0.6944\n",
      "Epoch 191/250\n",
      "1727/1727 [==============================] - 0s 203us/step - loss: 0.6060 - accuracy: 0.6879 - val_loss: 0.6290 - val_accuracy: 0.6667\n",
      "Epoch 192/250\n",
      "1727/1727 [==============================] - 0s 214us/step - loss: 0.5827 - accuracy: 0.7024 - val_loss: 0.6187 - val_accuracy: 0.6759\n",
      "Epoch 193/250\n",
      "1727/1727 [==============================] - 0s 211us/step - loss: 0.5779 - accuracy: 0.7006 - val_loss: 0.6312 - val_accuracy: 0.6991\n",
      "Epoch 194/250\n",
      "1727/1727 [==============================] - 0s 204us/step - loss: 0.5887 - accuracy: 0.6902 - val_loss: 0.6256 - val_accuracy: 0.6991\n",
      "Epoch 195/250\n",
      "1727/1727 [==============================] - 0s 202us/step - loss: 0.5866 - accuracy: 0.7006 - val_loss: 0.6272 - val_accuracy: 0.7153\n",
      "Epoch 196/250\n",
      "1727/1727 [==============================] - 0s 202us/step - loss: 0.5944 - accuracy: 0.7001 - val_loss: 0.6408 - val_accuracy: 0.6435\n",
      "Epoch 197/250\n",
      "1727/1727 [==============================] - 0s 217us/step - loss: 0.5891 - accuracy: 0.6862 - val_loss: 0.6568 - val_accuracy: 0.6597\n",
      "Epoch 198/250\n",
      "1727/1727 [==============================] - 0s 205us/step - loss: 0.5999 - accuracy: 0.6827 - val_loss: 0.6194 - val_accuracy: 0.6898\n",
      "Epoch 199/250\n",
      "1727/1727 [==============================] - 0s 203us/step - loss: 0.5968 - accuracy: 0.6966 - val_loss: 0.6165 - val_accuracy: 0.7083\n",
      "Epoch 200/250\n",
      "1727/1727 [==============================] - 0s 223us/step - loss: 0.5926 - accuracy: 0.6891 - val_loss: 0.6187 - val_accuracy: 0.6991\n",
      "Epoch 201/250\n",
      "1727/1727 [==============================] - 0s 216us/step - loss: 0.5990 - accuracy: 0.6908 - val_loss: 0.6310 - val_accuracy: 0.6782\n",
      "Epoch 202/250\n",
      "1727/1727 [==============================] - 0s 209us/step - loss: 0.5835 - accuracy: 0.7024 - val_loss: 0.6401 - val_accuracy: 0.6667\n",
      "Epoch 203/250\n",
      "1727/1727 [==============================] - 0s 220us/step - loss: 0.5806 - accuracy: 0.7035 - val_loss: 0.6189 - val_accuracy: 0.6968\n",
      "Epoch 204/250\n",
      "1727/1727 [==============================] - 0s 205us/step - loss: 0.5940 - accuracy: 0.6943 - val_loss: 0.6113 - val_accuracy: 0.6921\n",
      "Epoch 205/250\n",
      "1727/1727 [==============================] - 0s 206us/step - loss: 0.5819 - accuracy: 0.6983 - val_loss: 0.6142 - val_accuracy: 0.6875\n",
      "Epoch 206/250\n",
      "1727/1727 [==============================] - 0s 217us/step - loss: 0.5863 - accuracy: 0.7035 - val_loss: 0.6186 - val_accuracy: 0.6481\n",
      "Epoch 207/250\n",
      "1727/1727 [==============================] - 0s 213us/step - loss: 0.5899 - accuracy: 0.6948 - val_loss: 0.6107 - val_accuracy: 0.6968\n",
      "Epoch 208/250\n",
      "1727/1727 [==============================] - 0s 214us/step - loss: 0.5832 - accuracy: 0.6896 - val_loss: 0.6512 - val_accuracy: 0.6690\n",
      "Epoch 209/250\n",
      "1727/1727 [==============================] - 0s 202us/step - loss: 0.6000 - accuracy: 0.6937 - val_loss: 0.6216 - val_accuracy: 0.6968\n",
      "Epoch 210/250\n",
      "1727/1727 [==============================] - 0s 202us/step - loss: 0.5913 - accuracy: 0.6867 - val_loss: 0.6176 - val_accuracy: 0.6875\n",
      "Epoch 211/250\n",
      "1727/1727 [==============================] - 0s 204us/step - loss: 0.5858 - accuracy: 0.6983 - val_loss: 0.6477 - val_accuracy: 0.6759\n",
      "Epoch 212/250\n",
      "1727/1727 [==============================] - 0s 207us/step - loss: 0.6097 - accuracy: 0.6809 - val_loss: 0.6269 - val_accuracy: 0.6944\n",
      "Epoch 213/250\n",
      "1727/1727 [==============================] - 0s 217us/step - loss: 0.5950 - accuracy: 0.6891 - val_loss: 0.6212 - val_accuracy: 0.6968\n",
      "Epoch 214/250\n",
      "1727/1727 [==============================] - 0s 204us/step - loss: 0.5795 - accuracy: 0.6995 - val_loss: 0.6379 - val_accuracy: 0.6759\n",
      "Epoch 215/250\n",
      "1727/1727 [==============================] - 0s 206us/step - loss: 0.5922 - accuracy: 0.7012 - val_loss: 0.6212 - val_accuracy: 0.6898\n",
      "Epoch 216/250\n",
      "1727/1727 [==============================] - 0s 213us/step - loss: 0.5933 - accuracy: 0.6925 - val_loss: 0.6201 - val_accuracy: 0.6806\n",
      "Epoch 217/250\n",
      "1727/1727 [==============================] - 0s 217us/step - loss: 0.5880 - accuracy: 0.6891 - val_loss: 0.6217 - val_accuracy: 0.6991\n",
      "Epoch 218/250\n",
      "1727/1727 [==============================] - 0s 212us/step - loss: 0.5800 - accuracy: 0.7018 - val_loss: 0.6149 - val_accuracy: 0.6991\n",
      "Epoch 219/250\n",
      "1727/1727 [==============================] - 0s 207us/step - loss: 0.5867 - accuracy: 0.6954 - val_loss: 0.6279 - val_accuracy: 0.6944\n",
      "Epoch 220/250\n",
      "1727/1727 [==============================] - 0s 213us/step - loss: 0.5972 - accuracy: 0.6833 - val_loss: 0.6318 - val_accuracy: 0.6782\n",
      "Epoch 221/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1727/1727 [==============================] - 0s 205us/step - loss: 0.5869 - accuracy: 0.6966 - val_loss: 0.6288 - val_accuracy: 0.7060\n",
      "Epoch 222/250\n",
      "1727/1727 [==============================] - 0s 218us/step - loss: 0.5851 - accuracy: 0.6954 - val_loss: 0.6198 - val_accuracy: 0.6921\n",
      "Epoch 223/250\n",
      "1727/1727 [==============================] - 0s 200us/step - loss: 0.5914 - accuracy: 0.6850 - val_loss: 0.6304 - val_accuracy: 0.6806\n",
      "Epoch 224/250\n",
      "1727/1727 [==============================] - 0s 200us/step - loss: 0.5900 - accuracy: 0.6948 - val_loss: 0.6404 - val_accuracy: 0.6759\n",
      "Epoch 225/250\n",
      "1727/1727 [==============================] - 0s 205us/step - loss: 0.5825 - accuracy: 0.7122 - val_loss: 0.6394 - val_accuracy: 0.6551\n",
      "Epoch 226/250\n",
      "1727/1727 [==============================] - 0s 208us/step - loss: 0.5822 - accuracy: 0.6925 - val_loss: 0.6581 - val_accuracy: 0.6736\n",
      "Epoch 227/250\n",
      "1727/1727 [==============================] - 0s 219us/step - loss: 0.5859 - accuracy: 0.6983 - val_loss: 0.6557 - val_accuracy: 0.6366\n",
      "Epoch 228/250\n",
      "1727/1727 [==============================] - 0s 210us/step - loss: 0.5966 - accuracy: 0.6920 - val_loss: 0.6270 - val_accuracy: 0.6806\n",
      "Epoch 229/250\n",
      "1727/1727 [==============================] - 0s 289us/step - loss: 0.5943 - accuracy: 0.6972 - val_loss: 0.6459 - val_accuracy: 0.6921\n",
      "Epoch 230/250\n",
      "1727/1727 [==============================] - 0s 269us/step - loss: 0.5858 - accuracy: 0.6896 - val_loss: 0.6489 - val_accuracy: 0.6458\n",
      "Epoch 231/250\n",
      "1727/1727 [==============================] - 0s 240us/step - loss: 0.5965 - accuracy: 0.6954 - val_loss: 0.6237 - val_accuracy: 0.6736\n",
      "Epoch 232/250\n",
      "1727/1727 [==============================] - 1s 317us/step - loss: 0.6004 - accuracy: 0.6873 - val_loss: 0.6256 - val_accuracy: 0.6968\n",
      "Epoch 233/250\n",
      "1727/1727 [==============================] - 0s 268us/step - loss: 0.5837 - accuracy: 0.6908 - val_loss: 0.6427 - val_accuracy: 0.6968\n",
      "Epoch 234/250\n",
      "1727/1727 [==============================] - 0s 254us/step - loss: 0.5870 - accuracy: 0.6995 - val_loss: 0.6545 - val_accuracy: 0.6759\n",
      "Epoch 235/250\n",
      "1727/1727 [==============================] - 0s 228us/step - loss: 0.5821 - accuracy: 0.7001 - val_loss: 0.6448 - val_accuracy: 0.6389\n",
      "Epoch 236/250\n",
      "1727/1727 [==============================] - 0s 239us/step - loss: 0.6000 - accuracy: 0.6815 - val_loss: 0.6245 - val_accuracy: 0.6829\n",
      "Epoch 237/250\n",
      "1727/1727 [==============================] - 0s 250us/step - loss: 0.5920 - accuracy: 0.6995 - val_loss: 0.6340 - val_accuracy: 0.6435\n",
      "Epoch 238/250\n",
      "1727/1727 [==============================] - 0s 274us/step - loss: 0.5938 - accuracy: 0.6954 - val_loss: 0.6212 - val_accuracy: 0.6968\n",
      "Epoch 239/250\n",
      "1727/1727 [==============================] - 0s 246us/step - loss: 0.5870 - accuracy: 0.6885 - val_loss: 0.6215 - val_accuracy: 0.6875\n",
      "Epoch 240/250\n",
      "1727/1727 [==============================] - 1s 301us/step - loss: 0.5945 - accuracy: 0.6977 - val_loss: 0.6183 - val_accuracy: 0.6898\n",
      "Epoch 241/250\n",
      "1727/1727 [==============================] - 1s 325us/step - loss: 0.5852 - accuracy: 0.6925 - val_loss: 0.6201 - val_accuracy: 0.6898\n",
      "Epoch 242/250\n",
      "1727/1727 [==============================] - 1s 298us/step - loss: 0.5838 - accuracy: 0.6983 - val_loss: 0.6483 - val_accuracy: 0.6875\n",
      "Epoch 243/250\n",
      "1727/1727 [==============================] - 1s 305us/step - loss: 0.5891 - accuracy: 0.6954 - val_loss: 0.6585 - val_accuracy: 0.6806\n",
      "Epoch 244/250\n",
      "1727/1727 [==============================] - 0s 245us/step - loss: 0.5888 - accuracy: 0.6937 - val_loss: 0.6309 - val_accuracy: 0.6944\n",
      "Epoch 245/250\n",
      "1727/1727 [==============================] - 0s 288us/step - loss: 0.5863 - accuracy: 0.7018 - val_loss: 0.6334 - val_accuracy: 0.6852\n",
      "Epoch 246/250\n",
      "1727/1727 [==============================] - 0s 259us/step - loss: 0.5940 - accuracy: 0.6885 - val_loss: 0.6282 - val_accuracy: 0.6898\n",
      "Epoch 247/250\n",
      "1727/1727 [==============================] - 0s 266us/step - loss: 0.5931 - accuracy: 0.6977 - val_loss: 0.6241 - val_accuracy: 0.6782\n",
      "Epoch 248/250\n",
      "1727/1727 [==============================] - 0s 218us/step - loss: 0.5910 - accuracy: 0.6920 - val_loss: 0.6413 - val_accuracy: 0.6435\n",
      "Epoch 249/250\n",
      "1727/1727 [==============================] - 1s 429us/step - loss: 0.5920 - accuracy: 0.6752 - val_loss: 0.6346 - val_accuracy: 0.6968\n",
      "Epoch 250/250\n",
      "1727/1727 [==============================] - 1s 373us/step - loss: 0.5924 - accuracy: 0.7041 - val_loss: 0.6263 - val_accuracy: 0.6829\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1a44b5d410>"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model7.fit(X_train7, y_train, batch_size=20, epochs=250, verbose=1, validation_data=(X_test7, y_test), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 features: accuracy= 63.66%\n"
     ]
    }
   ],
   "source": [
    "# estimate accuracy on 10 features for test data set\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"10 features: \"\"%s= %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 features: accuracy= 63.19%\n"
     ]
    }
   ],
   "source": [
    "# estimate accuracy on 15 features for test data set\n",
    "scores2 = model2.evaluate(X_test2, y_test, verbose=0)\n",
    "print(\"15 features: \"\"%s= %.2f%%\" % (model2.metrics_names[1], scores2[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 features: accuracy= 67.36%\n"
     ]
    }
   ],
   "source": [
    "# estimate accuracy on 20 features for test data set\n",
    "scores3 = model3.evaluate(X_test3, y_test, verbose=0)\n",
    "print(\"20 features: \"\"%s= %.2f%%\" % (model3.metrics_names[1], scores3[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 features: accuracy= 70.14%\n"
     ]
    }
   ],
   "source": [
    "# estimate accuracy on 25 features for test data set\n",
    "scores4 = model4.evaluate(X_test4, y_test, verbose=0)\n",
    "print(\"25 features: \"\"%s= %.2f%%\" % (model4.metrics_names[1], scores4[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 features: accuracy= 69.44%\n"
     ]
    }
   ],
   "source": [
    "# estimate accuracy on 30 features for test data set\n",
    "scores5 = model5.evaluate(X_test5, y_test, verbose=0)\n",
    "print(\"30 features: \"\"%s= %.2f%%\" % (model5.metrics_names[1], scores5[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 features: accuracy= 69.21%\n"
     ]
    }
   ],
   "source": [
    "# estimate accuracy on 35 features for test data set\n",
    "scores6 = model6.evaluate(X_test6, y_test, verbose=0)\n",
    "print(\"35 features: \"\"%s= %.2f%%\" % (model6.metrics_names[1], scores6[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 features: accuracy= 68.29%\n"
     ]
    }
   ],
   "source": [
    "# estimate accuracy on 35 features for test data set\n",
    "scores7 = model7.evaluate(X_test7, y_test, verbose=0)\n",
    "print(\"40 features: \"\"%s= %.2f%%\" % (model7.metrics_names[1], scores7[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = [63.66, 63.19, 67.36, 70.14, 69.44, 69.21, 68.29]\n",
    "pt = ['10','15','20','25','30','35','40']\n",
    "y_pos = np.arange(len(pt))\n",
    "\n",
    "plt.plot(y_pos, acc, color= (0.5,0.1,0.5,0.6))\n",
    "plt.title('K features comparison')\n",
    "plt.xlabel('K features')\n",
    "plt.ylabel('Accuracy in %)')\n",
    "plt.ylim(45,90)\n",
    "\n",
    "plt.xticks(y_pos,pt)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
